A wide variety of tasks in AI and optimal control can be formalized as sequential decision-making processes. Reinforcement learning (RL) is the study of approximating optimal decision-making in natural and artificial systems. Reinforcement learning can be subdivided into two fundamental problems: \textit{learning} and \textit{planning}\cite{b1}. We refer to the decision-making entity as the \textit{agent}, and everything outside of the \textit{agent} as its \textit{environment}. The goal of learning is for an agent to improve its \textit{policy} from its interactions with the environment. The goal of planning is for an agent to improve its policy without further interaction with the environment. Despite being different, these two problems are intimately related. During learning, the agent interacts with the environment by executing actions and observing their consequences. During planning, the agent can interact with a model of the environment by simulating actions and observing their consequences. At each time-step $t$, the agent receives observations $s_t \in S$ from its environment and executes an action $a_t \in A$ according to its behavior policy. The environment then provides a feedback signal in the form of a reward $r_{t+1} \in R$. This time series of actions, observations, and rewards defines the agent's \textit{experience}. In both cases (planning and learning), the agent updates its policy from its experience. The goal of RL is to improve the agent's future reward given its past experience\cite{b2}. 

The RL problem is deeply indebted to the idea of Markov decision processes (MDPs) from the field of optimal control. Since the game of Go is a classic combinatorial game (zero-sum, perfect information, deterministic, discrete, and sequential\cite{b17,b18}), it can be modeled as a fully observable finite MDP\cite{b2}. Most RL algorithms involve estimating \textit{value} functions -- functions of states (or of state-action pairs) that estimate how good is the agent's given state (or how good it is to perform a given action in a given state). The notion of `how good' is defined in terms of expected return that depends on the agent's action. Accordingly, value functions are defined with respect to the behavior, called policies. A \textit{policy}, $\pi(s_t, a_t)$ is a mapping from states to probabilities of selecting each legal action\cite{b2}. 

Solving RL problems means finding a policy that achieves maximum expected future rewards by selecting an optimal sequence of actions. For finite MDPs, there exists such an optimal policy. There may also exist more than one optimal policies but they share the same optimal state-value function, $v_*(s)$ (or optimal state-action value function, $q_*(s,a)$). A well-defined notion of optimality organizes the approach to \textit{learning} we described earlier. For the kind of task in which we were interested, developing an AI for Go, learning optimal policies can only be done with high computational cost and time. In tasks with small, finite state sets (such as tic-tac-toe), it is possible to form these approximations using tables with one entry for one state (or state-action pair). These we call the \textit{tabular} methods. In many cases of practical application, however, there are far more states (e.g., game-tree of Go) than could possibly be entries in a table. In these cases, the functions must be approximated using a more compact parameterized function representation. Thus our framing of the RL problem for the game of Go forces us to settle for approximations. That is where deep learning helps to approximate both policy and value functions. In particular, deep convolutional neural networks (DCNNs) are powerful at giving you higher-level representations of image data \cite{b19,b20}, and also accurately mimics predictions of a labelled data \cite{b21,b22,b23}. In AlphaGo, both policy and value functions are learned by reinforcement learning with function approximation provided by DCNNs. Hence the name deep reinforcement learning. It is worth noting that instead of deep reinforcement learning starting with randomly initialized weights, it started from weights learned from a large collection of human expert moves. More specifics on training these networks are discussed in Sec.~\ref{AlphaGo}.


% Decision theory (or planning) combines probability theory with utility theory to provide a formal and complete framework for decisions made under uncertainty. Game theory extends decision theory to situations in which multiple agents interact. A game can be defined as a set of established rules that allow the interaction of one or more players to produce specified outcomes. The following properties classify games: number of players involved, whether the reward to all players sums to zero (zero-sum), whether the state of the game is fully or partially observable to the players (information), whether uncertainty plays a part (determinism), whether actions are applied sequentially (sequential), and whether actions are discrete in real time (discrete). Games with two players that are zero-sum, perfect information, deterministic, discrete, and sequential are called combinatorial games (e.g., chess, Go). They have controlled environments defined by simple rules but typically exhibit deep and complex play, as amply demonstrated by Go.

% A Markov decision process (MDP) models RL problems in fully observable environments (like Go) using four components: $S$ (a set of states, with $s_0$ being the initial state), $A$ (a set of actions), $P(s,a,s')$ (a transition model determining the probability of reaching state $s'$ if action $a$ is applied to state $s$), and $R(s)$ (a reward function). Overall decisions are modeled as sequences of (state, action) pairs, in which a probability distribution decides the next state $s'$ given the current state $s$ and the chosen action $a$. In addition, the main objective is to find the policy $p$ that maps from states to actions, specifying which action will be chosen from each state $S$ that yields the highest expected reward.

% % Think about this section -- needs to be well built 
% Due to its generality, RL is studied in many disciplines, such as game theory, control theory, operations research, multi-agent systems, swarm intelligence, and statistics.\ RL is a computational approach to understanding and automating goal-directed learning (similar to sequential decision-making). RL uses the framework of MDP (Sec.~\ref{GT}) to define the interaction between a learning agent and its environment in terms of states, actions, and rewards. In this rich text of RL, there are two categories of solution methods: one caters to small and tractable state spaces, known as \textit{tabular} solution methods, the other caters to arbitrarily large state spaces (e.g., combinatorial games), known as \textit{approximate} solution methods. Deep reinforcement learning (DRL) algorithms incorporate deep learning to solve such intractable search space MDPs using policy gradient techniques of \textit{approximate} solution methods. DCNNs are powerful at giving you higher level representations of image data and also accurately mimics predictions of a labelled data. In AlphaGo, many board positions as a 19 x 19 image are passed through architectures of two DCNNs (policy and value networks respectively) to construct offline knowledge of the game of Go. More specifics on training these networks is discussed in Sec.~\ref{AlphaGo}.