Sequential decision-making is one of the fundamental problems of artificial intelligence (AI). The challenge is to get an agent to select a sequence of actions that maximizes a long-term objective (e.g., winning a game). Planning and search have been widely applied to solve such problems \cite{b1}. Planning is the computation process by which the agent updates its action selection policy $\pi(s_t, a_t)$, where $s_t \in S$ and $a_t \in A$ are state and action taken in state $s_t$ at time-step $t$, respectively. Search refers to the computation process used to select an action from a particular root state $s_0 \in S$. We adopt the definition of planning used typically in reinforcement learning \cite[pp.~1--11]{b2} and the definition of search often used in two-player games \cite[pp.~192--203]{b3}.

Game-playing research is interesting and essential for the AI community\cite{b4}. The reason is generally twofold: first, games provide cheap, reproducible environments suitable for testing new search algorithms, pattern-based evaluation methods, or learning concept, second, there is a \textit{human aspect} of artificial game playing: people like to challenge themselves (by machines) in their mental abilities. 
%This \textit{psychological layer} of a human-machine competition is crucial and cannot be overestimated. 
In this rivalry, the machines have already surpassed the top human players in several challenging games.

Classic two-player and perfect information games have been used as excellent testbeds. In games such as chess\cite{b5}, checkers\cite{b6}, Othello\cite{b7}, and backgammon\cite{b8}, human performance levels have been exceeded by programs that combine brute force tree-search\cite{b9} with human knowledge or reinforcement learning. However, before 2015, these approaches and others have failed to achieve equivalent performance in the game of Go, mainly due to its enormous search space. Furthermore, simple heuristics for evaluating the position, like those proven to be successful in chess, are hard to design for Go\cite{b10}. Also, there are long-term influences of the moves that affect the outcome of the game hundreds of moves later. Therefore, a Go program that must simultaneously cope with the enormous game complexity, long-term effects of moves and evaluate the strategic importance of positions posed a grand challenge. Hence, progress in computer Go was thought to ultimately contribute to advancing the field of AI as a whole\cite{b11}.

Silver et al.\cite{b12} have shown that two general principles can reduce this effective search space: reducing the depth, $d$, using a `value network,' and reducing breadth, $b$, using a `policy network.' The long-term effects of moves and evaluation of positions are tackled by combining these two networks with a novel Monte Carlo tree-search. This Go program is called \textit{AlphaGo}. These two deceptively simple key ingredients, Monte Carlo tree-search and convolutional neural networks, have helped AlphaGo achieve remarkable wins against top Go professional players. The latest version of AlphaGo, called \textit{MuZero}, has achieved superhuman performance in chess, Shogi, and visually complex domains of Atari games without knowing their underlying rules (along with Go) \cite{b13}. The ideas behind MuZero's powerful learning and planning algorithms may pave the way towards the long-standing ambition of AI to create programs that learn on their own from first principles. 